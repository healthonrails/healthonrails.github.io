{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69rHM_M3MamZ"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/princeton-vl/RAFT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fo6P-TUHOIfF"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PZzlKaJMhwB"
   },
   "outputs": [],
   "source": [
    "%cd RAFT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6V0BHnAMnjq"
   },
   "outputs": [],
   "source": [
    "!./download_models.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRGLF3o2MptN"
   },
   "outputs": [],
   "source": [
    "!python demo.py --model=models/raft-things.pth --path=demo-frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mC91fo09NLzN"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NY0uDlr3Oe6w"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('core')\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from raft import RAFT\n",
    "from utils import flow_viz\n",
    "from utils.utils import InputPadder\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab.patches import  cv2_imshow\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkfjYO8-iTm8"
   },
   "outputs": [],
   "source": [
    "!unzip /content/video.zip -d /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3qgcdSVN6qm"
   },
   "outputs": [],
   "source": [
    "def viz(img, flo):\n",
    "    img = img[0].permute(1,2,0).cpu().numpy()\n",
    "    flo = flo[0].permute(1,2,0).cpu().numpy()\n",
    "    \n",
    "    # map flow to rgb image\n",
    "    flo = flow_viz.flow_to_image(flo)\n",
    "    img_flo = np.concatenate([img, flo], axis=0)\n",
    "    #plt.imshow(img_flo / 255.0)\n",
    "    #plt.show()\n",
    "    cv2_imshow(img_flo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9QrytkSOcdL"
   },
   "outputs": [],
   "source": [
    "def load_image(imfile):\n",
    "    img = np.array(Image.open(imfile)).astype(np.uint8)\n",
    "    img = torch.from_numpy(img).permute(2, 0, 1).float()\n",
    "    return img[None].to(DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_k_aN2I2Ohrf"
   },
   "outputs": [],
   "source": [
    "def demo(args):\n",
    "    model = torch.nn.DataParallel(RAFT(args))\n",
    "    model.load_state_dict(torch.load(args.model))\n",
    "\n",
    "    model = model.module\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images = glob.glob(os.path.join(args.path, '*.png')) + \\\n",
    "                 glob.glob(os.path.join(args.path, '*.jpg'))\n",
    "        \n",
    "        images = sorted(images)\n",
    "        for imfile1, imfile2 in zip(images[:-1], images[1:]):\n",
    "            image1 = load_image(imfile1)\n",
    "            print(image1.shape)\n",
    "            image2 = load_image(imfile2)\n",
    "            padder = InputPadder(image1.shape)\n",
    "            image1, image2 = padder.pad(image1, image2)\n",
    "            print(image1.shape,image2.shape)\n",
    "\n",
    "            flow_low, flow_up = model(image1, image2, iters=20, test_mode=True)\n",
    "            viz(image1, flow_up)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UiZ2qBkXLnp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pn7mWWTbVCww"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', default=\"models/raft-things.pth\",help=\"restore checkpoint\")\n",
    "parser.add_argument('--path', default=\"/content/video\", help=\"dataset for evaluation\")\n",
    "parser.add_argument('--small', action='store_true', help='use small model')\n",
    "parser.add_argument('--mixed_precision', action='store_true', help='use mixed precision')\n",
    "parser.add_argument('--alternate_corr', action='store_true', help='use efficent correlation implementation')\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfmC8AjkSh8e"
   },
   "outputs": [],
   "source": [
    "demo(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrbqoYxhSzf3"
   },
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(RAFT(args))\n",
    "model.load_state_dict(torch.load(args.model))\n",
    "\n",
    "model = model.module\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMf9BuVQXRbd"
   },
   "outputs": [],
   "source": [
    "VIDEO_PATH = '/content/video.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSHj9lnYXw4S"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torchvision import transforms, utils\n",
    "class VideoFrameDataset(IterableDataset):\n",
    "    \"\"\"Video Frame dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, video_file, root_dir=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_file (string): Path to the video file.\n",
    "            root_dir (string): Directory with all the videos.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.video_file = video_file\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.cap = cv2.VideoCapture(self.video_file)\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        ret, old_frame = self.cap.read()\n",
    "        num_frames = (int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "        old_frame = cv2.cvtColor(old_frame, cv2.COLOR_BGR2RGB)\n",
    "        for num in range(num_frames - 1):\n",
    "            ret,frame = self.cap.read()\n",
    "            #frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            #old_gray = frame_gray.copy()\n",
    "            \n",
    "\n",
    "            if self.transform:\n",
    "                x = self.transform(old_frame)\n",
    "                y = self.transform(frame)\n",
    "            else:\n",
    "                x = old_frame\n",
    "                y = frame\n",
    "            old_frame = frame.copy()\n",
    "\n",
    "            yield x, y\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMNG0qNaYijK"
   },
   "outputs": [],
   "source": [
    "video_dataset = VideoFrameDataset(VIDEO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJrFc-LUYqJQ"
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(video_dataset,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ToYbkWiYtQ4"
   },
   "outputs": [],
   "source": [
    "FRAMES = 1000\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    for image1,image2 in loader:\n",
    "        image1 = image1.permute(0,3,1,2).float()\n",
    "        image2 = image2.permute(0,3,1,2).float()\n",
    "        \n",
    "        #image1[None].to(DEVICE)\n",
    "        #image2[None].to(DEVICE)\n",
    "        image1 = image1.cuda()\n",
    "        image2 = image2.cuda()\n",
    "        padder = InputPadder(image1.shape)\n",
    "        image1, image2 = padder.pad(image1, image2)\n",
    "        flow_low, flow_up = model(image1, image2, iters=20, test_mode=True)\n",
    "        viz(image1, flow_up)\n",
    "        if counter == FRAMES:\n",
    "            break\n",
    "        counter += 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RAFT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}