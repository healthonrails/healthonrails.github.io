{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZtQQyOgelW5"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/healthonrails/annolid/blob/main/docs/tutorials/PointRend_in_Detectron2_Tutorial.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHnVupBBn9eR"
   },
   "source": [
    "# \"[PointRend](https://arxiv.org/abs/1912.08193) in Detectron2\" Tutorial\n",
    "\n",
    "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
    "\n",
    "Welcome to the [PointRend project](https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend) in detectron2! In this tutorial, we will go through some basics usage of PointRend, including the following:\n",
    "* Run inference on images or videos, with an existing PointRend model\n",
    "* Look into PointRend internal representation.\n",
    "\n",
    "You can make a copy of this tutorial or use \"File -> Open in playground mode\" to play with it yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "# Install detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_FzH13EjseR"
   },
   "outputs": [],
   "source": [
    "# install dependencies: \n",
    "!pip install pyyaml==5.1\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "!gcc --version\n",
    "# opencv is pre-installed on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUmK767kWQlq"
   },
   "outputs": [],
   "source": [
    "# install detectron2: (Colab has CUDA 10.1 + torch 1.8)\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
    "import torch\n",
    "assert torch.__version__.startswith(\"1.8\")   # need to manually install torch 1.8 if Colab changes its default version\n",
    "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
    "# exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1QQpW-Msqnf"
   },
   "outputs": [],
   "source": [
    "# clone the repo in order to access pre-defined configs\n",
    "!git clone --branch v0.3 https://github.com/facebookresearch/detectron2.git detectron2_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyAvNCJMmvFF"
   },
   "outputs": [],
   "source": [
    "# You may need to restart your runtime prior to this, to let your installation take effect\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog\n",
    "coco_metadata = MetadataCatalog.get(\"coco_2017_val\")\n",
    "\n",
    "# import PointRend project\n",
    "from detectron2.projects import point_rend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdIK_wzhrydb"
   },
   "source": [
    "## Upload a labeled COCO format dataset as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-w-hdUHDrxDb"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "dataset =  list(uploaded.keys())[0]  #\"/content/dataset.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rz6vwjopulPV"
   },
   "outputs": [],
   "source": [
    "# please uncomment and change the dataset location in case your dataset is\n",
    "# already upload to colab\n",
    "#dataset = \"/content/novelctrlk6_8_coco_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnO3VMUzO5-h"
   },
   "outputs": [],
   "source": [
    "!unzip $dataset -d /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk4gID50K03a"
   },
   "source": [
    "# Run a pre-trained PointRend model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgKyUL4pngvE"
   },
   "source": [
    "We first download an image from the COCO dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dq9GY37ml1kr"
   },
   "outputs": [],
   "source": [
    "\n",
    "im = cv2.imread(\"/content/novelctrlk6_8_coco_dataset/train/JPEGImages/00001980_61.jpg\")\n",
    "cv2_imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM1thbN-ntjI"
   },
   "source": [
    "Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image. First, we make a prediction using a standard Mask R-CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkPgSoYvVAEe"
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "mask_rcnn_predictor = DefaultPredictor(cfg)\n",
    "mask_rcnn_outputs = mask_rcnn_predictor(im)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7sw7uz9Vyo-"
   },
   "source": [
    "Now, we load a PointRend model and show its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUjkwRsOn1O0"
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "# Add PointRend-specific config\n",
    "point_rend.add_pointrend_config(cfg)\n",
    "# Load a config from file\n",
    "cfg.merge_from_file(\"detectron2_repo/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.15  # set threshold for this model\n",
    "# Use a model from PointRend model zoo: https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend#pretrained-models\n",
    "cfg.MODEL.WEIGHTS = \"detectron2://PointRend/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco/164955410/model_final_3c3198.pkl\"\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IRGo8d0qkgR"
   },
   "outputs": [],
   "source": [
    "# Show and compare two predictions: \n",
    "v = Visualizer(im[:, :, ::-1], coco_metadata, scale=1.2, instance_mode=ColorMode.IMAGE_BW)\n",
    "mask_rcnn_result = v.draw_instance_predictions(mask_rcnn_outputs[\"instances\"].to(\"cpu\")).get_image()\n",
    "v = Visualizer(im[:, :, ::-1], coco_metadata, scale=1.2, instance_mode=ColorMode.IMAGE_BW)\n",
    "point_rend_result = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\")).get_image()\n",
    "print(\"Mask R-CNN with PointRend (top)     vs.     Default Mask R-CNN (bottom)\")\n",
    "cv2_imshow(np.concatenate((point_rend_result, mask_rcnn_result), axis=0)[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QWiQ9nETI15"
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = f\"{os.path.basename(dataset).split('_')[0]}\" \n",
    "DATASET_DIR = f\"{dataset.replace('.zip','')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mmZAnYrS-mC"
   },
   "outputs": [],
   "source": [
    "# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "register_coco_instances(f\"{DATASET_NAME}_train\", {}, f\"{DATASET_DIR}/train/annotations.json\", f\"{DATASET_DIR}/train/\")\n",
    "register_coco_instances(f\"{DATASET_NAME}_valid\", {}, f\"{DATASET_DIR}/valid/annotations.json\", f\"{DATASET_DIR}/valid/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0xBcd_bTYhV"
   },
   "outputs": [],
   "source": [
    "from detectron2.data import get_detection_dataset_dicts\n",
    "dataset_dicts = get_detection_dataset_dicts([f\"{DATASET_NAME}_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4maZCWOTsvz"
   },
   "outputs": [],
   "source": [
    "_dataset_metadata = MetadataCatalog.get(f\"{DATASET_NAME}_train\")\n",
    "_dataset_metadata\n",
    "_dataset_metadata.thing_classes\n",
    "_dataset_metadata.thing_dataset_id_to_contiguous_id\n",
    "_dataset_metadata.thing_colors = coco_metadata.thing_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFruYEfVUU1U"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(_dataset_metadata.thing_classes)\n",
    "print(f\"{NUM_CLASSES} Number of classes in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hk8BEejUaEg"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=_dataset_metadata, scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoS-aOTTUlqb"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPbJolUDUiTu"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lyEmUE7Uq6T"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "cfg = get_cfg()\n",
    "point_rend.add_pointrend_config(cfg)\n",
    "cfg.merge_from_file(\"detectron2_repo/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\")\n",
    "cfg.DATASETS.TRAIN = (f\"{DATASET_NAME}_train\",)\n",
    "cfg.DATASETS.TEST = (f\"{DATASET_NAME}_valid\")\n",
    "cfg.DATALOADER.NUM_WORKERS = 2 #@param\n",
    "cfg.DATALOADER.SAMPLER_TRAIN = \"RepeatFactorTrainingSampler\"\n",
    "cfg.DATALOADER.REPEAT_THRESHOLD = 0.3\n",
    "#cfg.MODEL.WEIGHTS = \"detectron2://PointRend/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco/164955410/model_final_3c3198.pkl\"\n",
    "cfg.SOLVER.IMS_PER_BATCH =  4#@param\n",
    "cfg.SOLVER.BASE_LR = 0.0025 #@param # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 2000 #@param    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128 #@param   # faster, and good enough for this toy dataset (default: 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LXIIY4oWrZx"
   },
   "outputs": [],
   "source": [
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES  #  (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
    "cfg.MODEL.POINT_HEAD.NUM_CLASSES = NUM_CLASSES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7Fei4YpWv7W"
   },
   "outputs": [],
   "source": [
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LL5-1kEtXKYg"
   },
   "outputs": [],
   "source": [
    "# Look at training curves in tensorboard:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GVW_UgUXTiX"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcFYbnp5YtwV"
   },
   "outputs": [],
   "source": [
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "#cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "#cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.15   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Viaj3POyhybG"
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "dataset_dicts = get_detection_dataset_dicts([f\"{DATASET_NAME}_valid\"])\n",
    "for d in dataset_dicts:    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=_dataset_metadata, \n",
    "                   scale=0.5, \n",
    "                   instance_mode=ColorMode.SEGMENTATION   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7euLYqBh4OM"
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(f\"{DATASET_NAME}_valid\", cfg, False, output_dir=\"/content/eval_output/\")\n",
    "val_loader = build_detection_test_loader(cfg, f\"{DATASET_NAME}_valid\")\n",
    "print(inference_on_dataset(trainer.model, val_loader, evaluator))\n",
    "# another equivalent way to evaluate the model is to use `trainer.test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x18y4yOPn4Dn"
   },
   "outputs": [],
   "source": [
    "VIDEO_INPUT=\"/content/novelobjectK6Fcontrol.mkv\" #@param {type: \"string\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWdmfTDQh7_-"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "video = cv2.VideoCapture(VIDEO_INPUT)\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frames_per_second = video.get(cv2.CAP_PROP_FPS)\n",
    "num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "basename = os.path.basename(VIDEO_INPUT)\n",
    "OUTPUT_DIR = \"/content/eval_output\"\n",
    "import os \n",
    "os.makedirs(OUTPUT_DIR,exist_ok=True)\n",
    "output_fname = os.path.join(OUTPUT_DIR, basename)\n",
    "output_fname = os.path.splitext(output_fname)[0] + \"_mask_tracked.mp4\"\n",
    "output_file = cv2.VideoWriter(\n",
    "                filename=output_fname,\n",
    "                # some installation of opencv may not support x264 (due to its license),\n",
    "                # you can try other format (e.g. MPEG)\n",
    "                fourcc=cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "                fps=float(frames_per_second),\n",
    "                frameSize=(width, height),\n",
    "                isColor=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67EOSS-qiRlJ"
   },
   "outputs": [],
   "source": [
    "def _frame_from_video(video):\n",
    "  while video.isOpened():\n",
    "      success, frame = video.read()\n",
    "      if success:\n",
    "          yield frame\n",
    "      else:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CepnUdQ6iUFr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pycocotools.mask as mask_util\n",
    "class_names = _dataset_metadata.thing_classes\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2crUiQ7icqL"
   },
   "outputs": [],
   "source": [
    "frame_number = 0\n",
    "tracking_results = []\n",
    "VIS = True\n",
    "for frame in _frame_from_video(video): \n",
    "    im = frame\n",
    "    outputs = predictor(im)\n",
    "    out_dict = {}  \n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    num_instance = len(instances)\n",
    "    if num_instance == 0:\n",
    "        out_dict['frame_number'] = frame_number\n",
    "        out_dict['x1'] = None\n",
    "        out_dict['y1'] = None\n",
    "        out_dict['x2'] = None\n",
    "        out_dict['y2'] = None\n",
    "        out_dict['instance_name'] = None\n",
    "        out_dict['class_score'] = None\n",
    "        out_dict['segmentation'] = None\n",
    "        tracking_results.append(out_dict)\n",
    "        out_dict = {}\n",
    "    else:\n",
    "        boxes = instances.pred_boxes.tensor.numpy()\n",
    "        boxes = boxes.tolist()\n",
    "        scores = instances.scores.tolist()\n",
    "        classes = instances.pred_classes.tolist()\n",
    "\n",
    "        has_mask = instances.has(\"pred_masks\")\n",
    "\n",
    "        if has_mask:\n",
    "            rles =[\n",
    "                   mask_util.encode(np.array(mask[:,:,None], order=\"F\", dtype=\"uint8\"))[0]\n",
    "                   for mask in instances.pred_masks\n",
    "            ]\n",
    "            for rle in rles:\n",
    "              rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
    "\n",
    "        assert len(rles) == len(boxes)\n",
    "        for k in range(num_instance):\n",
    "            box = boxes[k]\n",
    "            out_dict['frame_number'] = frame_number\n",
    "            out_dict['x1'] = box[0]\n",
    "            out_dict['y1'] = box[1]\n",
    "            out_dict['x2'] = box[2]\n",
    "            out_dict['y2'] = box[3]\n",
    "            out_dict['instance_name'] = class_names[classes[k]]\n",
    "            out_dict['class_score'] = scores[k]\n",
    "            out_dict['segmentation'] = rles[k]\n",
    "            if frame_number % 1000 == 0:\n",
    "              print(f\"Frame number {frame_number}: {out_dict}\")\n",
    "            tracking_results.append(out_dict)\n",
    "            out_dict = {}\n",
    "        \n",
    "    # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    if VIS:\n",
    "        v = Visualizer(im[:, :, ::-1],\n",
    "                    metadata=_dataset_metadata, \n",
    "                    scale=0.5, \n",
    "                    instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "         )\n",
    "        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "        out_image = out.get_image()[:, :, ::-1]\n",
    "        output_file.write(out_image)\n",
    "        if frame_number % 1000 == 0:\n",
    "            cv2_imshow(out_image)\n",
    "    frame_number += 1\n",
    "    print(f\"Processing frame number {frame_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFTV5OGcidlM"
   },
   "outputs": [],
   "source": [
    "video.release()\n",
    "output_file.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63W-_tLzioci"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tracking_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1zbRa-KitCY"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIXs8AE9iuYr"
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZCiFcTui97E"
   },
   "outputs": [],
   "source": [
    "df_top = df.groupby(['frame_number','instance_name'],sort=False).head(1)\n",
    "df_top.head()\n",
    "tracking_results_csv = \"/content/con_01_pointrend_tracking_results_with_segmenation.csv\" #@param\n",
    "df_top.to_csv(tracking_results_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPKTWi-IMDMk"
   },
   "source": [
    "# Visualize PointRend point sampling process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxtfjespMah-"
   },
   "source": [
    "In this section we show how PointRend's point sampling process works. To do this, we need to access intermediate representations of the predictor `model.forward(...)` function. Thus, we run forward step manually copying the code step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XArt11tUGT2"
   },
   "outputs": [],
   "source": [
    "# First we define a simple function to help us plot the intermediate representations.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mask(mask, title=\"\", point_coords=None, figsize=10, point_marker_size=5):\n",
    "  '''\n",
    "  Simple plotting tool to show intermediate mask predictions and points \n",
    "  where PointRend is applied.\n",
    "  \n",
    "  Args:\n",
    "    mask (Tensor): mask prediction of shape HxW\n",
    "    title (str): title for the plot\n",
    "    point_coords ((Tensor, Tensor)): x and y point coordinates\n",
    "    figsize (int): size of the figure to plot\n",
    "    point_marker_size (int): marker size for points\n",
    "  '''\n",
    "\n",
    "  H, W = mask.shape\n",
    "  plt.figure(figsize=(figsize, figsize))\n",
    "  if title:\n",
    "    title += \", \"\n",
    "  plt.title(\"{}resolution {}x{}\".format(title, H, W), fontsize=30)\n",
    "  plt.ylabel(H, fontsize=30)\n",
    "  plt.xlabel(W, fontsize=30)\n",
    "  plt.xticks([], [])\n",
    "  plt.yticks([], [])\n",
    "  plt.imshow(mask, interpolation=\"nearest\", cmap=plt.get_cmap('gray'))\n",
    "  if point_coords is not None:\n",
    "    plt.scatter(x=point_coords[0], y=point_coords[1], color=\"red\", s=point_marker_size, clip_on=True) \n",
    "  plt.xlim(-0.5, W - 0.5)\n",
    "  plt.ylim(H - 0.5, - 0.5)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrzdyCaws5Mo"
   },
   "source": [
    "With `predictor` and `im` loaded in the previous section we run backbone, bounding box prediction, and coarse mask segmenation head. We visualize mask prediction for the foreground plane on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-meq_L0xLhr0"
   },
   "outputs": [],
   "source": [
    "from detectron2.data import transforms as T\n",
    "model = predictor.model\n",
    "# In this image we detect several objects but show only the first one.\n",
    "instance_idx = 0\n",
    "# Mask predictions are class-specific, \"plane\" class has id 4.\n",
    "category_idx = 4\n",
    "\n",
    "with torch.no_grad():\n",
    "  # Prepare input image.\n",
    "  height, width = im.shape[:2]\n",
    "  im_transformed = T.ResizeShortestEdge(800, 1333).get_transform(im).apply_image(im)\n",
    "  batched_inputs = [{\"image\": torch.as_tensor(im_transformed).permute(2, 0, 1)}]\n",
    "\n",
    "  # Get bounding box predictions first to simplify the code.\n",
    "  detected_instances = [x[\"instances\"] for x in model.inference(batched_inputs)]\n",
    "  [r.remove(\"pred_masks\") for r in detected_instances]  # remove existing mask predictions\n",
    "  pred_boxes = [x.pred_boxes for x in detected_instances] \n",
    "\n",
    "  # Run backbone.\n",
    "  images = model.preprocess_image(batched_inputs)\n",
    "  features = model.backbone(images.tensor)\n",
    "  \n",
    "  # Given the bounding boxes, run coarse mask prediction head.\n",
    "  mask_coarse_logits = model.roi_heads._forward_mask_coarse(features, pred_boxes)\n",
    "\n",
    "  plot_mask(\n",
    "      mask_coarse_logits[instance_idx, category_idx].to(\"cpu\"),\n",
    "      title=\"Coarse prediction\"\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHK4aDIxG_oT"
   },
   "outputs": [],
   "source": [
    "# Prepare features maps to use later\n",
    "mask_features_list = [\n",
    "  features[k] for k in model.roi_heads.mask_point_in_features\n",
    "]\n",
    "features_scales = [\n",
    "  model.roi_heads._feature_scales[k] \n",
    "  for k in model.roi_heads.mask_point_in_features\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdSy4Zvas9lt"
   },
   "source": [
    "### Point sampling during training\n",
    "\n",
    "During training we select points where coarse prediction is uncertain to train PointRend head. See section 3.1 in the PointRend [paper](https://arxiv.org/abs/1912.08193) for more details.\n",
    "\n",
    "To visualize different sampling strategy change `oversample_ratio` and `importance_sample_ratio` parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIny-6sotDCL"
   },
   "outputs": [],
   "source": [
    "from detectron2.projects.point_rend.roi_heads import calculate_uncertainty\n",
    "from detectron2.projects.point_rend.point_features import get_uncertain_point_coords_with_randomness\n",
    "\n",
    "# Change number of points to select\n",
    "num_points = 14 * 14\n",
    "# Change randomness parameters \n",
    "oversample_ratio = 3  # `k` in the paper\n",
    "importance_sample_ratio = 0.75  # `\\beta` in the paper\n",
    "\n",
    "with torch.no_grad():\n",
    "  # We take predicted classes, whereas during real training ground truth classes are used.\n",
    "  pred_classes = torch.cat([x.pred_classes for x in detected_instances])\n",
    "\n",
    "  # Select points given a corse prediction mask\n",
    "  point_coords = get_uncertain_point_coords_with_randomness(\n",
    "    mask_coarse_logits,\n",
    "    lambda logits: calculate_uncertainty(logits, pred_classes),\n",
    "    num_points=num_points,\n",
    "    oversample_ratio=oversample_ratio,\n",
    "    importance_sample_ratio=importance_sample_ratio\n",
    "  )\n",
    "\n",
    "  H, W = mask_coarse_logits.shape[-2:]\n",
    "  plot_mask(\n",
    "    mask_coarse_logits[instance_idx, category_idx].to(\"cpu\"),\n",
    "    title=\"Sampled points over the coarse prediction\",\n",
    "    point_coords=(\n",
    "      W * point_coords[instance_idx, :, 0].to(\"cpu\") - 0.5,\n",
    "      H * point_coords[instance_idx, :, 1].to(\"cpu\") - 0.5\n",
    "    ),\n",
    "    point_marker_size=50\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IP6fQaafs2He"
   },
   "source": [
    "### Point sampling during inference\n",
    "\n",
    "Starting from a 7x7 coarse prediction we bilinearly upsample it `num_subdivision_steps` times. At each step we find `num_subdivision_points` most uncertain points and make predictions for them using the PointRend head. See section 3.1 in the [paper](https://arxiv.org/abs/1912.08193) to know more details.\n",
    "\n",
    "Change `num_subdivision_steps` and `num_subdivision_points` parameters to change inference behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhyYdj3psMni"
   },
   "outputs": [],
   "source": [
    "from detectron2.layers import interpolate\n",
    "from detectron2.projects.point_rend.roi_heads import calculate_uncertainty\n",
    "from detectron2.projects.point_rend.point_features import (\n",
    "    get_uncertain_point_coords_on_grid,\n",
    "    point_sample,\n",
    "    point_sample_fine_grained_features,\n",
    ")\n",
    "\n",
    "num_subdivision_steps = 5\n",
    "num_subdivision_points = 28 * 28\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  plot_mask(\n",
    "      mask_coarse_logits[0, category_idx].to(\"cpu\").numpy(), \n",
    "      title=\"Coarse prediction\"\n",
    "  )\n",
    "\n",
    "  mask_logits = mask_coarse_logits\n",
    "  for subdivions_step in range(num_subdivision_steps):\n",
    "    # Upsample mask prediction\n",
    "    mask_logits = interpolate(\n",
    "        mask_logits, scale_factor=2, mode=\"bilinear\", align_corners=False\n",
    "    )\n",
    "    # If `num_subdivision_points` is larger or equalt to the\n",
    "    # resolution of the next step, then we can skip this step\n",
    "    H, W = mask_logits.shape[-2:]\n",
    "    if (\n",
    "      num_subdivision_points >= 4 * H * W\n",
    "      and subdivions_step < num_subdivision_steps - 1\n",
    "    ):\n",
    "      continue\n",
    "    # Calculate uncertainty for all points on the upsampled regular grid\n",
    "    uncertainty_map = calculate_uncertainty(mask_logits, pred_classes)\n",
    "    # Select most `num_subdivision_points` uncertain points\n",
    "    point_indices, point_coords = get_uncertain_point_coords_on_grid(\n",
    "        uncertainty_map, \n",
    "        num_subdivision_points\n",
    "    )\n",
    "\n",
    "    # Extract fine-grained and coarse features for the points\n",
    "    fine_grained_features, _ = point_sample_fine_grained_features(\n",
    "      mask_features_list, features_scales, pred_boxes, point_coords\n",
    "    )\n",
    "    coarse_features = point_sample(mask_coarse_logits, point_coords, align_corners=False)\n",
    "\n",
    "    # Run PointRend head for these points\n",
    "    point_logits = model.roi_heads.mask_point_head(fine_grained_features, coarse_features)\n",
    "\n",
    "    # put mask point predictions to the right places on the upsampled grid.\n",
    "    R, C, H, W = mask_logits.shape\n",
    "    x = (point_indices[instance_idx] % W).to(\"cpu\")\n",
    "    y = (point_indices[instance_idx] // W).to(\"cpu\")\n",
    "    point_indices = point_indices.unsqueeze(1).expand(-1, C, -1)\n",
    "    mask_logits = (\n",
    "      mask_logits.reshape(R, C, H * W)\n",
    "      .scatter_(2, point_indices, point_logits)\n",
    "      .view(R, C, H, W)\n",
    "    )\n",
    "    plot_mask(\n",
    "      mask_logits[instance_idx, category_idx].to(\"cpu\"), \n",
    "      title=\"Subdivision step: {}\".format(subdivions_step + 1),\n",
    "      point_coords=(x, y)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWCMM2tV6z6b"
   },
   "source": [
    "We can visualize mask prediction obtained in the previous block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWGAw75W6NSn"
   },
   "outputs": [],
   "source": [
    "from detectron2.modeling import GeneralizedRCNN\n",
    "from detectron2.modeling.roi_heads.mask_head import mask_rcnn_inference\n",
    "\n",
    "results = detected_instances\n",
    "mask_rcnn_inference(mask_logits, results)\n",
    "results = GeneralizedRCNN._postprocess(results, batched_inputs, images.image_sizes)[0]\n",
    "\n",
    "# We can use `Visualizer` to draw the predictions on the image.\n",
    "v = Visualizer(im_transformed[:, :, ::-1], coco_metadata)\n",
    "v = v.draw_instance_predictions(results[\"instances\"].to(\"cpu\"))\n",
    "cv2_imshow(v.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFqeMeF8RsQm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PointRend in Detectron2 Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}